{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "import datetime as dt\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nmslib\n",
    "# from transformers import pipeline\n",
    "\n",
    "# uncomment for downloading spacy models\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_lg\n",
    "# give it 2 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "def extract_entities(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == 'ORG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATN = pd.read_csv('data/all-the-news-2-1.csv')\n",
    "# 7min\n",
    "\n",
    "ATN_c = pd.read_csv('data/ATN_stripped2020.csv')\n",
    "# ATN_c = pd.read_csv('data/ATN_cleaned.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>section</th>\n",
       "      <th>publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>A Man Admitted To Trying To Crash A Train Into...</td>\n",
       "      <td>The journalists at BuzzFeed News are proud to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buzzfeed News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>Ruth Bader Ginsburg Still Working Out with Tra...</td>\n",
       "      <td>Here's some good news we can all use ... Ruth ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TMZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>Coronavirus Victim: 24-Year-Old Silvia Deyanir...</td>\n",
       "      <td>The journalists at BuzzFeed News are proud to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buzzfeed News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>Daily Telegraph Gives Chinese Coronavirus Prop...</td>\n",
       "      <td>The journalists at BuzzFeed News are proud to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buzzfeed News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>Coronavirus Pandemic: An American Is Trapped I...</td>\n",
       "      <td>Trenton Thurber The journalists at BuzzFeed Ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buzzfeed News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title   \n",
       "0  2020-04-02  A Man Admitted To Trying To Crash A Train Into...  \\\n",
       "1  2020-04-01  Ruth Bader Ginsburg Still Working Out with Tra...   \n",
       "2  2020-04-01  Coronavirus Victim: 24-Year-Old Silvia Deyanir...   \n",
       "3  2020-04-01  Daily Telegraph Gives Chinese Coronavirus Prop...   \n",
       "4  2020-04-01  Coronavirus Pandemic: An American Is Trapped I...   \n",
       "\n",
       "                                             article section    publication  \n",
       "0  The journalists at BuzzFeed News are proud to ...     NaN  Buzzfeed News  \n",
       "1  Here's some good news we can all use ... Ruth ...     NaN            TMZ  \n",
       "2  The journalists at BuzzFeed News are proud to ...     NaN  Buzzfeed News  \n",
       "3  The journalists at BuzzFeed News are proud to ...     NaN  Buzzfeed News  \n",
       "4  Trenton Thurber The journalists at BuzzFeed Ne...     NaN  Buzzfeed News  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATN_c.head(5)\n",
    "# ATN_stripped.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_clean = pd.read_csv('data/SP500_tickers_clean.csv')\n",
    "ticker_associations = pd.read_csv('data/SP500_ticker_associations.csv', header=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preprocess**\n",
    "\n",
    "**For articles**  \n",
    "remove stopwords and punctuation\n",
    "only keep the relevant columns (Date, Title, Article)\n",
    "\n",
    "**For ticker associations**  \n",
    "remove stopwords and punctuation\n",
    "concatenate and join words to one string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for articels and titles\n",
    "ATN_c['title'] = ATN_c['title'].str.lower().str.replace('[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                                                  2020-03-30\n",
       "title          intel chipmakers may skip big layoffs because ...\n",
       "article        This story is available exclusively on Busines...\n",
       "section                                                      NaN\n",
       "publication                                     Business Insider\n",
       "Name: 1128, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATN_c.iloc[1128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Names</th>\n",
       "      <th>Associations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MO</td>\n",
       "      <td>Altria:Altria Group</td>\n",
       "      <td>Marlboro:Copenhagen:Juul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon Inc:Amazon.com</td>\n",
       "      <td>Amazon Prime:Kindle:Alexa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMCR</td>\n",
       "      <td>Amcor:Amcor PLC</td>\n",
       "      <td>Plastic packaging:Resilient packaging</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker                  Names                           Associations\n",
       "0     MO    Altria:Altria Group               Marlboro:Copenhagen:Juul\n",
       "1   AMZN  Amazon Inc:Amazon.com              Amazon Prime:Kindle:Alexa\n",
       "2   AMCR        Amcor:Amcor PLC  Plastic packaging:Resilient packaging"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_associations.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the ticker, name, association df into a list of \"documents\", where each ticker has concatenated all its names and associations\n",
    "ticker_docs = ticker_associations[['Ticker', 'Names', 'Associations']].astype(str).apply(' '.join, axis=1)\n",
    "ticker_docs = ticker_docs.str.replace(':', ' ').str.lower().str.replace('[^\\w\\s]', '', regex=True)\n",
    "# remove stop words\n",
    "ticker_docs = ticker_docs.apply(lambda x: ' '.join([word for word in x.split() if word not in (STOP_WORDS)]))\n",
    "# only include each word once per line\n",
    "# ticker_docs = ticker_docs.apply(lambda x: ' '.join(set(x.split())))\n",
    "# ticker_docs = ticker_associations['Names'].astype(str).str.lower().str.replace(':', ' ').str.replace('[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        mo altria altria group marlboro copenhagen juul\n",
       "1      amzn amazon inc amazoncom amazon prime kindle ...\n",
       "2      amcr amcor amcor plc plastic packaging resilie...\n",
       "3            amd advanced micro devices amd ryzen radeon\n",
       "4      aee ameren corporation ameren illinois ameren ...\n",
       "                             ...                        \n",
       "478                       aes aes corporation aes energy\n",
       "479    agilent technologies inc agilent gcms instruments\n",
       "480    akam akamai technologies inc akamai intelligen...\n",
       "481                 algn align technology inc invisalign\n",
       "482            ko cocacola company cocacola sprite fanta\n",
       "Length: 483, dtype: object"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here, each ticker and assoc. is a document, ready to be vectorized\n",
    "ticker_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_docs = ATN_c['title'].astype(str).str.lower().str.replace('[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **vectorize with TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), analyzer='word') # 3/4-grams for char-level\n",
    "ticker_association_vectors = vectorizer.fit_transform(article_docs.tolist())\n",
    "# 17min"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Indexing vectors with NMSLIB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "*******************************************************"
     ]
    }
   ],
   "source": [
    "index = nmslib.init(method='hnsw', space='cosinesimil_sparse', data_type=nmslib.DataType.SPARSE_VECTOR, dtype=nmslib.DistType.FLOAT)\n",
    "# make the ticker vectors dense\n",
    "# t_a_v = ticker_association_vectors.todense()\n",
    "index.addDataPointBatch(ticker_association_vectors)\n",
    "index.createIndex({'post': 2}, print_progress=True)\n",
    "index.setQueryTimeParams({'efSearch': 300, 'algoType': 'old'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_lemmatized = ATN_c['title'].iloc[1128:1129].apply(lemmatize_text)\n",
    "# doc = nlp(ATN_c.article.iloc[1128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'intel' in ATN_c['title'].iloc[1128].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(ticker_association_vectors[0])\n",
    "# vectorizer.vocabulary_\n",
    "# Get the first document in the sparse matrix\n",
    "doc_vector = ticker_association_vectors[226]\n",
    "\n",
    "# Convert the sparse matrix row to a dense array\n",
    "doc_array = doc_vector.toarray().flatten()\n",
    "\n",
    "# Get the vocabulary and create a reverse mapping from index to term\n",
    "vocab = vectorizer.vocabulary_\n",
    "reverse_vocab = {index: term for term, index in vocab.items()}\n",
    "\n",
    "# Create a list of tuples (term, score) and sort it by score in descending order\n",
    "scores = [(reverse_vocab[i], score) for i, score in enumerate(doc_array)]\n",
    "sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 10 terms by TF-IDF score\n",
    "for term, score in sorted_scores[:10]:\n",
    "    print(f\"{term}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "knnQueryBatch(): incompatible function arguments. The following argument types are supported:\n    1. (self: nmslib.dist.FloatIndex, queries: object, k: int = 10, num_threads: int = 0) -> object\n\nInvoked with: <nmslib.FloatIndex method='hnsw' space='cosinesimil_sparse' at 0x7f7df2e2b5e0>, <nmslib.FloatIndex method='hnsw' space='cosinesimil_sparse' at 0x7f7df2e2b5e0>, <1x1763856 sparse matrix of type '<class 'numpy.float64'>'\n\twith 2 stored elements in Compressed Sparse Row format>; kwargs: k=10, num_threads=4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[213], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m article_vector \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform([\u001b[39m'\u001b[39m\u001b[39mintel corporation\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[39m# article_dense = article_vector.todense().astype('float32')[0]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# article_vector = article_vector.astype('float32') #! HAS TO BE FLOAT32 for c++ conversion\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[39m# Find the 5 nearest neighbors in the index.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# indices, distances = index.knnQuery(vector=article_vector, k=10)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m indices, distances \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mknnQueryBatch(index, article_vector, k\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, num_threads\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: knnQueryBatch(): incompatible function arguments. The following argument types are supported:\n    1. (self: nmslib.dist.FloatIndex, queries: object, k: int = 10, num_threads: int = 0) -> object\n\nInvoked with: <nmslib.FloatIndex method='hnsw' space='cosinesimil_sparse' at 0x7f7df2e2b5e0>, <nmslib.FloatIndex method='hnsw' space='cosinesimil_sparse' at 0x7f7df2e2b5e0>, <1x1763856 sparse matrix of type '<class 'numpy.float64'>'\n\twith 2 stored elements in Compressed Sparse Row format>; kwargs: k=10, num_threads=4"
     ]
    }
   ],
   "source": [
    "# Let's say `article` is the text of an article you want to analyze.\n",
    "# article_vector = vectorizer.transform(article_lemmatized.tolist())\n",
    "article_vector = vectorizer.transform(['intel corporation'])\n",
    "# article_dense = article_vector.todense().astype('float32')[0]\n",
    "# article_vector = article_vector.astype('float32') #! HAS TO BE FLOAT32 for c++ conversion\n",
    "\n",
    "# Find the 5 nearest neighbors in the index.\n",
    "# indices, distances = index.knnQuery(vector=article_vector, k=10)\n",
    "indices, distances = index.knnQueryBatch(index, article_vector, k=10, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corporation: 0.7374060750007629\n",
      "intel: 0.6754496693611145\n",
      "00: 0.0\n",
      "00 mm: 0.0\n",
      "00 mm 13: 0.0\n",
      "00 mm 14: 0.0\n",
      "00 qq: 0.0\n",
      "00 qq 11: 0.0\n",
      "00 qq reuters: 0.0\n",
      "0000: 0.0\n",
      "0000 senior: 0.0\n",
      "0000 senior notes: 0.0\n",
      "00003: 0.0\n",
      "0008: 0.0\n",
      "0008 per: 0.0\n",
      "0008 per share: 0.0\n",
      "001: 0.0\n",
      "001 as: 0.0\n",
      "001 as fed: 0.0\n",
      "0012: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Convert the sparse matrix row to a dense array\n",
    "doc_array = article_vector.toarray().flatten()\n",
    "\n",
    "# Get the vocabulary and create a reverse mapping from index to term\n",
    "vocab = vectorizer.vocabulary_\n",
    "reverse_vocab = {index: term for term, index in vocab.items()}\n",
    "\n",
    "# Create a list of tuples (term, score) and sort it by score in descending order\n",
    "scores = [(reverse_vocab[i], score) for i, score in enumerate(doc_array)]\n",
    "sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 10 terms by TF-IDF score\n",
    "for term, score in sorted_scores[:20]:\n",
    "    print(f\"{term}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "# Filter out the results that are below a certain threshold.\n",
    "filtered_indices = [i for i, d in zip(indices, distances) if d > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.809136  , 0.887751  , 0.8990367 , 0.9172862 , 0.94079536,\n",
       "       0.94723755, 0.9586839 , 0.9589516 , 0.9598353 , 0.9607994 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([article_lemmatized.iloc[i] for i in indices])\n",
    "# article_lemmatized.iloc[indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BKR', 'MCD', 'MGM', 'PH', 'QCOM', 'MTCH', 'LVS']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'intel chipmakers may skip big layoffs because demand will snap back'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([ticker_associations.iloc[i]['Ticker'] for i in filtered_indices])\n",
    "ATN_c.iloc[1128]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('production and', 6113),\n",
       " ('wall street', 7565),\n",
       " ('lam research', 4751),\n",
       " ('mo altria', 5240),\n",
       " ('altria altria', 265)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_scores = np.squeeze(np.asarray(article_vector))\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "sorted_ngrams = sorted(vocabulary.items(), key=lambda x: tfidf_scores[x[1]], reverse=True)\n",
    "sorted_ngrams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('e ', 0.1685521), (' t', 0.15543188), ('th', 0.13725233), (' th', 0.1311991), (' a', 0.12768699), ('he ', 0.10906077), ('ma', 0.105170794), ('the', 0.09971617), ('t ', 0.099340156), ('re', 0.099340156)]\n"
     ]
    }
   ],
   "source": [
    "# Reverse the vocabulary dictionary\n",
    "index_to_ngram = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "# Get the indices of the n-grams with the highest TF-IDF scores\n",
    "top_ngram_indices = np.argsort(tfidf_scores)[::-1]\n",
    "\n",
    "# Get the n-grams corresponding to these indices\n",
    "top_ngrams = [(index_to_ngram[i], tfidf_scores[i]) for i in top_ngram_indices]\n",
    "\n",
    "# Print the top 20 n-grams\n",
    "print(top_ngrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"the assumption here is that 'Apple' (or term) is a term that exists in the vocabulary of your TF-IDF vectorizer.\"\n",
    "What do we do if the term/ticker we are searching for doesnt exist in the vocabulary?..\n",
    "\n",
    "TF-IDF vectorizer uses unigrams (whole words) as default. Do we want this behaviour? \n",
    "there's also a way to set multiple ngrams for wider search i guess.: TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating data structures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dates = articles.set_index('id')['date'].to_dict() # TODO: get back here when i figured storing out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There needs to be a way to keep track of which articles mention what tickers, and the articles date and ID. \n",
    "Then each article can have its sentiment score calculated and for each ticker and day, we can avg out the sentiment.\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStrategy: \\npreprocess to remove redundant words: stopwords, punctuation, lowercase.\\nWe might need NER for having multi word company names...\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Strategy: \n",
    "preprocess to remove redundant words: stopwords, punctuation, lowercase.\n",
    "We might need NER for having multi word company names... or we do some magic with the alt names\n",
    "\n",
    "https://bergvca.github.io/2017/10/14/super-fast-string-matching.html : provides some super fast name matching using cosine similarity on n-grams from TF-IDF\n",
    "\n",
    "Each documents is an article, so we find the ngrams that are rare across documents but some documents then have a high TF. \n",
    "This way we would know that document is talking about our ngram. ???\n",
    "\n",
    "each artcle is a document\n",
    "TF-IDF vectorizer from sklearn\n",
    "feed into NMSLIB\n",
    "setup articles with ID and date attached? (can it be done?)\n",
    "for each ticker, find the closest articles.\n",
    "create dict with ticker key, list of articles as values.\n",
    "for each mentioned article, create a sentiment score using FinBERT\n",
    "for each ticker, create a pd.series that accumulates the sentiment scores of all articles for a day into 1 row in the series.\n",
    "we now have 500 series, each with all dates in the period of our original article dataset\n",
    "each series has a exp.decay func applied to each day, so it computes a new weighted score for the day based on the score of the last 2 weeks\n",
    "each accumulated score column can be combined for all tickers, which should be transformed into long format for use with the financial dataset\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 3-grams in \"McDonalds\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['McDo', 'cDon', 'Dona', 'onal', 'nald', 'alds']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngrams(string, n=3):\n",
    "    string = re.sub(r'[^\\w\\s]',r'', string) # remove all non-words\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "print('All 3-grams in \"McDonalds\":')\n",
    "ngrams('McDonalds', n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of articles: 611924452, len of ngrams: 592302728\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "ll = 0\n",
    "for article in ATN_stripped.article:\n",
    "    l += len(article)\n",
    "    ll += len(ngrams(article, n=3))\n",
    "\n",
    "print(f'len of articles: {l}, len of ngrams: {ll}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of articles: 12929260, len of ngrams: 12046816\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "ll = 0\n",
    "for article in ATN_stripped.title:\n",
    "    l += len(article)\n",
    "    ll += len(ngrams(article, n=4))\n",
    "\n",
    "print(f'len of articles: {l}, len of ngrams: {ll}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189978"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATN_stripped.__len__()\n",
    "ATN_publishers = ATN_stripped.publication.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.383333333333334"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.7*190/60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
