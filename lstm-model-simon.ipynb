{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Load in the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Open</th>\n",
       "      <th>Low</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Sector</th>\n",
       "      <th>log_return_3m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>MMM</td>\n",
       "      <td>148.050003</td>\n",
       "      <td>145.399994</td>\n",
       "      <td>148.320007</td>\n",
       "      <td>146.820007</td>\n",
       "      <td>3277200</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>MMM</td>\n",
       "      <td>146.820007</td>\n",
       "      <td>145.610001</td>\n",
       "      <td>147.500000</td>\n",
       "      <td>147.460007</td>\n",
       "      <td>2688100</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>MMM</td>\n",
       "      <td>145.589996</td>\n",
       "      <td>143.419998</td>\n",
       "      <td>145.759995</td>\n",
       "      <td>144.490005</td>\n",
       "      <td>2997100</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>MMM</td>\n",
       "      <td>142.520004</td>\n",
       "      <td>140.630005</td>\n",
       "      <td>143.130005</td>\n",
       "      <td>140.970001</td>\n",
       "      <td>3553500</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-08</th>\n",
       "      <td>MMM</td>\n",
       "      <td>141.360001</td>\n",
       "      <td>140.220001</td>\n",
       "      <td>142.500000</td>\n",
       "      <td>140.490005</td>\n",
       "      <td>2664000</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Ticker        Open         Low  ...   Volume       Sector  log_return_3m\n",
       "Date                                       ...                                     \n",
       "2016-01-04    MMM  148.050003  145.399994  ...  3277200  Industrials            NaN\n",
       "2016-01-05    MMM  146.820007  145.610001  ...  2688100  Industrials            NaN\n",
       "2016-01-06    MMM  145.589996  143.419998  ...  2997100  Industrials            NaN\n",
       "2016-01-07    MMM  142.520004  140.630005  ...  3553500  Industrials            NaN\n",
       "2016-01-08    MMM  141.360001  140.220001  ...  2664000  Industrials            NaN\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_stock_data = pd.read_csv('data/SP500_stock_prices_cleaned_with_3month_return.csv', index_col=0, parse_dates=True)\n",
    "stock_tickers = pd.Series(raw_stock_data.Ticker.unique())\n",
    "\n",
    "raw_stock_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preprocess the dataframe**\n",
    "Turn the dataframe into the ticker dictionary of feature and target tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "from data_processing import DataFrame_to_Tensors, FindNearestDateIndex, SplitData, CreateSequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM\n",
      "AOS\n",
      "ABT\n",
      "ABBV\n",
      "ACN\n",
      "ATVI\n",
      "ADM\n",
      "ADBE\n",
      "ADP\n",
      "AAP\n",
      "AES\n",
      "AFL\n",
      "A\n",
      "APD\n",
      "AKAM\n",
      "ALK\n",
      "ALB\n",
      "ARE\n",
      "ALGN\n",
      "ALLE\n",
      "LNT\n",
      "ALL\n",
      "GOOGL\n",
      "GOOG\n",
      "MO\n",
      "AMZN\n",
      "AMCR\n",
      "AMD\n",
      "AEE\n",
      "AAL\n",
      "AEP\n",
      "AXP\n",
      "AIG\n",
      "AMT\n",
      "AWK\n",
      "AMP\n",
      "ABC\n",
      "AME\n",
      "AMGN\n",
      "APH\n",
      "ADI\n",
      "ANSS\n",
      "AON\n",
      "APA\n",
      "AAPL\n",
      "AMAT\n",
      "APTV\n",
      "ACGL\n",
      "ANET\n",
      "AJG\n",
      "AIZ\n",
      "T\n",
      "ATO\n",
      "ADSK\n",
      "AZO\n",
      "AVB\n",
      "AVY\n",
      "BKR\n",
      "BALL\n",
      "BAC\n",
      "BBWI\n",
      "BAX\n",
      "BDX\n",
      "WRB\n",
      "BBY\n",
      "BIO\n",
      "TECH\n",
      "BIIB\n",
      "BLK\n",
      "BK\n",
      "BA\n",
      "BKNG\n",
      "BWA\n",
      "BXP\n",
      "BSX\n",
      "BMY\n",
      "AVGO\n",
      "BR\n",
      "BRO\n",
      "BG\n",
      "CHRW\n",
      "CDNS\n",
      "CZR\n",
      "CPT\n",
      "CPB\n",
      "COF\n",
      "CAH\n",
      "KMX\n",
      "CCL\n",
      "CTLT\n",
      "CAT\n",
      "CBOE\n",
      "CBRE\n",
      "CDW\n",
      "CE\n",
      "CNC\n",
      "CNP\n",
      "CF\n",
      "CRL\n",
      "SCHW\n",
      "CHTR\n",
      "CVX\n",
      "CMG\n",
      "CB\n",
      "CHD\n",
      "CI\n",
      "CINF\n",
      "CTAS\n",
      "CSCO\n",
      "C\n",
      "CFG\n",
      "CLX\n",
      "CME\n",
      "CMS\n",
      "KO\n",
      "CTSH\n",
      "CL\n",
      "CMCSA\n",
      "CMA\n",
      "CAG\n",
      "COP\n",
      "ED\n",
      "STZ\n",
      "COO\n",
      "CPRT\n",
      "GLW\n",
      "CSGP\n",
      "COST\n",
      "CTRA\n",
      "CCI\n",
      "CSX\n",
      "CMI\n",
      "CVS\n",
      "DHI\n",
      "DHR\n",
      "DRI\n",
      "DVA\n",
      "DE\n",
      "DAL\n",
      "XRAY\n",
      "DVN\n",
      "DXCM\n",
      "FANG\n",
      "DLR\n",
      "DFS\n",
      "DISH\n",
      "DIS\n",
      "DG\n",
      "DLTR\n",
      "D\n",
      "DPZ\n",
      "DOV\n",
      "DTE\n",
      "DUK\n",
      "DD\n",
      "DXC\n",
      "EMN\n",
      "ETN\n",
      "EBAY\n",
      "ECL\n",
      "EIX\n",
      "EW\n",
      "EA\n",
      "ELV\n",
      "LLY\n",
      "EMR\n",
      "ENPH\n",
      "ETR\n",
      "EOG\n",
      "EPAM\n",
      "EQT\n",
      "EFX\n",
      "EQIX\n",
      "EQR\n",
      "ESS\n",
      "EL\n",
      "ETSY\n",
      "RE\n",
      "EVRG\n",
      "ES\n",
      "EXC\n",
      "EXPE\n",
      "EXPD\n",
      "EXR\n",
      "XOM\n",
      "FFIV\n",
      "FDS\n",
      "FICO\n",
      "FAST\n",
      "FRT\n",
      "FDX\n",
      "FITB\n",
      "FRC\n",
      "FSLR\n",
      "FE\n",
      "FIS\n",
      "FISV\n",
      "FLT\n",
      "FMC\n",
      "F\n",
      "FTNT\n",
      "BEN\n",
      "FCX\n",
      "GRMN\n",
      "IT\n",
      "GEN\n",
      "GNRC\n",
      "GD\n",
      "GE\n",
      "GIS\n",
      "GM\n",
      "GPC\n",
      "GILD\n",
      "GL\n",
      "GPN\n",
      "GS\n",
      "HAL\n",
      "HIG\n",
      "HAS\n",
      "HCA\n",
      "PEAK\n",
      "HSIC\n",
      "HSY\n",
      "HES\n",
      "HPE\n",
      "HLT\n",
      "HOLX\n",
      "HD\n",
      "HON\n",
      "HRL\n",
      "HST\n",
      "HPQ\n",
      "HUM\n",
      "HBAN\n",
      "HII\n",
      "IBM\n",
      "IEX\n",
      "IDXX\n",
      "ITW\n",
      "ILMN\n",
      "INCY\n",
      "PODD\n",
      "INTC\n",
      "ICE\n",
      "IFF\n",
      "IP\n",
      "IPG\n",
      "INTU\n",
      "ISRG\n",
      "IVZ\n",
      "IQV\n",
      "IRM\n",
      "JBHT\n",
      "JKHY\n",
      "J\n",
      "JNJ\n",
      "JCI\n",
      "JPM\n",
      "JNPR\n",
      "K\n",
      "KDP\n",
      "KEY\n",
      "KEYS\n",
      "KMB\n",
      "KIM\n",
      "KMI\n",
      "KLAC\n",
      "KHC\n",
      "KR\n",
      "LHX\n",
      "LH\n",
      "LRCX\n",
      "LVS\n",
      "LDOS\n",
      "LEN\n",
      "LNC\n",
      "LIN\n",
      "LYV\n",
      "LKQ\n",
      "LMT\n",
      "L\n",
      "LOW\n",
      "LYB\n",
      "MTB\n",
      "MRO\n",
      "MPC\n",
      "MKTX\n",
      "MAR\n",
      "MMC\n",
      "MLM\n",
      "MAS\n",
      "MA\n",
      "MTCH\n",
      "MKC\n",
      "MCD\n",
      "MCK\n",
      "MDT\n",
      "MRK\n",
      "META\n",
      "MET\n",
      "MTD\n",
      "MGM\n",
      "MCHP\n",
      "MU\n",
      "MSFT\n",
      "MAA\n",
      "MHK\n",
      "MOH\n",
      "TAP\n",
      "MDLZ\n",
      "MPWR\n",
      "MNST\n",
      "MCO\n",
      "MS\n",
      "MOS\n",
      "MSI\n",
      "MSCI\n",
      "NDAQ\n",
      "NTAP\n",
      "NFLX\n",
      "NWL\n",
      "NEM\n",
      "NWSA\n",
      "NWS\n",
      "NEE\n",
      "NKE\n",
      "NI\n",
      "NDSN\n",
      "NSC\n",
      "NTRS\n",
      "NOC\n",
      "NCLH\n",
      "NRG\n",
      "NUE\n",
      "NVDA\n",
      "NVR\n",
      "NXPI\n",
      "ORLY\n",
      "OXY\n",
      "ODFL\n",
      "OMC\n",
      "ON\n",
      "OKE\n",
      "ORCL\n",
      "PCAR\n",
      "PKG\n",
      "PARA\n",
      "PH\n",
      "PAYX\n",
      "PAYC\n",
      "PYPL\n",
      "PNR\n",
      "PEP\n",
      "PKI\n",
      "PFE\n",
      "PCG\n",
      "PM\n",
      "PSX\n",
      "PNW\n",
      "PXD\n",
      "PNC\n",
      "POOL\n",
      "PPG\n",
      "PPL\n",
      "PFG\n",
      "PG\n",
      "PGR\n",
      "PLD\n",
      "PRU\n",
      "PEG\n",
      "PTC\n",
      "PSA\n",
      "PHM\n",
      "QRVO\n",
      "PWR\n",
      "QCOM\n",
      "DGX\n",
      "RL\n",
      "RJF\n",
      "RTX\n",
      "O\n",
      "REG\n",
      "REGN\n",
      "RF\n",
      "RSG\n",
      "RMD\n",
      "RHI\n",
      "ROK\n",
      "ROL\n",
      "ROP\n",
      "ROST\n",
      "RCL\n",
      "SPGI\n",
      "CRM\n",
      "SBAC\n",
      "SLB\n",
      "STX\n",
      "SEE\n",
      "SRE\n",
      "NOW\n",
      "SHW\n",
      "SPG\n",
      "SWKS\n",
      "SJM\n",
      "SNA\n",
      "SEDG\n",
      "SO\n",
      "LUV\n",
      "SWK\n",
      "SBUX\n",
      "STT\n",
      "STLD\n",
      "STE\n",
      "SYK\n",
      "SYF\n",
      "SNPS\n",
      "SYY\n",
      "TMUS\n",
      "TROW\n",
      "TTWO\n",
      "TPR\n",
      "TRGP\n",
      "TGT\n",
      "TEL\n",
      "TDY\n",
      "TFX\n",
      "TER\n",
      "TSLA\n",
      "TXN\n",
      "TXT\n",
      "TMO\n",
      "TJX\n",
      "TSCO\n",
      "TT\n",
      "TDG\n",
      "TRV\n",
      "TRMB\n",
      "TFC\n",
      "TYL\n",
      "TSN\n",
      "USB\n",
      "UDR\n",
      "ULTA\n",
      "UNP\n",
      "UAL\n",
      "UPS\n",
      "URI\n",
      "UNH\n",
      "UHS\n",
      "VLO\n",
      "VTR\n",
      "VRSN\n",
      "VRSK\n",
      "VZ\n",
      "VRTX\n",
      "VFC\n",
      "VTRS\n",
      "V\n",
      "VMC\n",
      "WAB\n",
      "WBA\n",
      "WMT\n",
      "WBD\n",
      "WM\n",
      "WAT\n",
      "WEC\n",
      "WFC\n",
      "WELL\n",
      "WST\n",
      "WDC\n",
      "WRK\n",
      "WY\n",
      "WHR\n",
      "WMB\n",
      "WTW\n",
      "GWW\n",
      "WYNN\n",
      "XEL\n",
      "XYL\n",
      "YUM\n",
      "ZBRA\n",
      "ZBH\n",
      "ZION\n",
      "ZTS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create the data sequences for each stock ticker.\n",
    "# we make \n",
    "data_sequences_dict = {}\n",
    "for ticker in stock_tickers:\n",
    "    print(ticker)\n",
    "    df = raw_stock_data[raw_stock_data['Ticker'] == ticker].drop(columns=['Ticker', 'Sector'])\n",
    "    data_sequences_dict[ticker] = DataFrame_to_Tensors(df, split_date='2019-10-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([879, 63, 6]), torch.Size([879]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sequences_dict['AAPL']['train_features'].shape, data_sequences_dict['AAPL']['train_targets'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and test split functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Build the model architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_dim, num_layers, output_dim, dropout):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # Define the dimensions of the LSTM layer\n",
    "        self.hidden_dim = hidden_layer_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_layer_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # fc output layer\n",
    "        self.fc = nn.Linear(hidden_layer_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Init hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Decode the last generated hidden state\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(6, 32, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# based on a paper.: See notion.\n",
    "\n",
    "input_dim, hidden_layer_dim, num_layers, output_dim, dropout = 6, 32, 2, 1, 0.4\n",
    "\n",
    "model = LSTM(input_dim, hidden_layer_dim, num_layers, output_dim, dropout)\n",
    "print(model)\n",
    "\n",
    "criteria = nn.MSELoss()\n",
    "\n",
    "# Well rounded, and doesnt need much fine-tuning.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Could have better convergence and generalization, but would take some fine-tuning to find optimal parameters.\n",
    "# we will have to see how long training takes, and decide if we can afford to do a search for hyperparameters \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
